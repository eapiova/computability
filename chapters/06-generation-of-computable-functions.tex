\chapter {Generation of computable functions}

We can prove that certain functions are computable if they are simpler combinations of functions that are computable. 
We have the set $\mathcal{C}$ of the computable functions and if we take $f_1, f_2 \in \mathcal{C}$ and compose them with a suitable operation $ op(f_1, f_2) $ we are still in the set $\mathcal{C}$.

More precisely we will prove that the $\mathcal{C}$ class is closed with respect to the following operations:
\begin{itemize}
\item (generalized) composition 
\item primitive recursion
\item (unbounded) minimization 
\end{itemize}

So to prove that the function $f:\nat^k\rightarrow \nat$ is computable 
we could write the URM program P that computes $f$ (such that $f_P^{(k)} = f$), 
or we could use the theorems of the closure of $\mathcal{C}$.

Actually the three operations we consider are not chosen randomly; 
the long term objective is to show that $\mathcal{C}$ coincides with the class of functions which can be obtained
through composition, primitive recursion and minimization, 
starting from a restricted core of basic functions (\textbf{partial recursive functions} of Godel-Kleene).

\section {Basic computable functions}
Consider the following basic functions
\begin{enumerate}
\item zero constant 
      \begin{align*}
        z: &\ \nat^k \rightarrow \nat\\
           & (x_1,\dots, x_k) \mapsto 0
      \end{align*}
\item successor 
      \begin{align*}
        s: &\ \nat \rightarrow \nat\\
        & x \mapsto x + 1
      \end{align*}
\item projection 
      \begin{align*}
        U_i^k: &\ \nat^k \rightarrow \nat\\
        & (x_1,\dots, x_k) \mapsto x_i
      \end{align*}
      
\end{enumerate}

\begin{remark}
  Identity is a sub-case of projection.
\end{remark}


The functions that compute these basic functions are the arithmetic instructions
\begin{enumerate}
\item $z$ computed by $Z(1)$;
\item $s$ computed by $S(1)$;
\item $U_i^k$ computed by $T(i, 1)$.
\end{enumerate}

To prove the properties of closure we will need to ``combine'' programs so we need a bit of notation.
\begin{notation}
  Given a URM program $P$
  \begin{itemize}
    \item $\rho(P)$ is the \textbf{maximum register index} used by $P$
    \item $ l(P) $ is the \textbf{number of instructions} in P;
    \item $P$ is in \textbf{standard form} if, for each $J(m,n,t)$ instruction, $t\leq l(P)+1$ (if it ends, it does so at the instruction $l(P)+1$).
  \end{itemize}
\end{notation}



Considering only standard form programs is not limitative, as stated by the following lemma:
\begin{lemma}
  For each URM program $P$ there exists an equivalent program $P'$ in standard form, i.e. for all $ k$, $f_p^{(k)} = f_{P'}^{(k)}$
\begin{proof} It is enough to replace every instruction $J(m,n,t)$ in $P$ such that $t>l(P)+1$ with $J(m,n,l(P)+1)$
\end{proof}
\end{lemma}

Often we will have to \textbf{concatenate} programs, let $P, Q$ be programs, 
the concatenation consists in executing $P$ and when it terminates executing $Q$, 
that is, every jump instruction $J(m,n,t)$ in $Q$ is replaced with $J(m,n,t+l(P))$

\begin{remark}
  If $P$ and $Q$ are in standard form then $PQ$ is in standard form; moreover $(PQ)R = P(QR)$. We will assume every program is in standard form and we will use concatenation freely.
\end{remark} 
It will be useful to take the input and give the output in arbitrary registers. 
Given a program $P$, we want a program $P[i_1,\dots,i_k \rightarrow h]$ that takes input from $R_{i1},\dots,R_{ik}$, puts the output in $ R_h $ without assuming that the rest of the registers are set to 0. 
This is easily obtainable with transfer and reset operations to move the contents of registers from $i_1,\dots,i_k$ to $1,\dots,k$ and the output from $h$ to 1.

$P[i_1,\dots,i_k \rightarrow h]$ is as follows:

$\begin{tabu}{l}
  T(i_1, 1)\\
  \dots    \\     
  T(i_k, k)\\          
  Z(k+1) \\
  \dots  \\
  Z(\rho(P))     \\     
  P \\
  T(1, l)
\end{tabu}$

\section {Generalized composition}
\begin{definition}
  Given a function
$f: \nat^k \rightarrow \nat$ and functions
$g_1,\dots,g_k: \nat^n \rightarrow \nat$
we define the \textbf{composition} $h: \nat^n \rightarrow \nat$ by 
  \begin{equation*}
    h(\vec{x}) = \begin{cases}
      f(g_1(\vec{x}), \dots, g_k(\vec{x})) & \mbox{if } g_1(\vec{x})\downarrow, \dots, g_k(\vec{x})\downarrow \mbox{ and } f(g_1(\vec{x}), \dots, g_k(\vec{x}))\downarrow\\
      \uparrow & \mbox{otherwise}
    \end{cases}
  \end{equation*}
\end{definition}

\begin{example}
  Consider
  \begin{equation*}
    z(x)=0\quad \forall x \qquad \varnothing(x)\uparrow\quad \forall x
  \end{equation*}
  then
  \begin{equation*}
    z(\varnothing(x))\uparrow \quad \forall x
  \end{equation*}
\end{example}

\begin{example}
  Consider $\varnothing$ and $U^2_1$, then
  \begin{equation*}
    U^2_1(x_1, x_2)=x_1 \quad \text{but} \quad U^2_1(x_1, \varnothing(x_2))\uparrow
  \end{equation*}
\end{example}

\begin{proposition}
  $\mathcal{C}$ is closed under generalised composition
  \begin{proof}
    Let
    \begin{align*}
      f& :\nat^k\rightarrow\nat\\
      g_1,\dots,g_k& : \nat^n\rightarrow\nat
    \end{align*}
    in $\mathcal{C}$, consider the composition
    \begin{align*}
      h :\ & \nat^k\rightarrow\nat\\
      & \vec{x} \mapsto f(g_1(\vec{x}), \dots, g_k(\vec{x}))
    \end{align*}
    Since $f, g_1,\dots,g_k\in\mathcal{C}$,
    we can take $F, G_1, \dots, G_k$ programs in standard form for them.

    Let us consider a register index $m = max\{\rho(F),\rho(G_1), \dots \rho(G_k),k,n\}$ not used, the program for the composition can be


$\begin{tabu}{|c|c|c|c|c|c|c|c|c|}
  \hline
  1                      & \dots                        & m                                 & m+1   & \dots & m+n   & m+n+1 & \dots & m+n+k \\
  \hline
  \multicolumn{3}{|c|}{\dots} & x_1 & \dots & x_n & g_1(\vec{x}) & \dots & g_k(\vec{x})                                                 \\
  \hline
\end{tabu}$

$\begin{tabu}{l}
  T(1, m+1)\\ 
  \dots\\
  T(n, m+n)\\
  G_1 [m+1,\dots,m+n \rightarrow m+n+1] \\
  \dots                                 \\
  G_k [m+1,\dots,m+n\rightarrow m+n+k]  \\
  F[m+n+1,\dots,m+n+k \rightarrow 1]
\end{tabu}$

then $h\in\mathcal{C}$.
  \end{proof}
\end{proposition}

\iffalse

SISTEMARE

\textbf{Corollary:} Let $f:\nat^k\rightarrow \nat$ be computable. Then $g:\nat^n\rightarrow \nat$, where $g(x_1,\dots,x_n) = f(x_{i1},\dots,x_{ik})$ is computable, where $(x_{i1},\dots,x_{ik})$ is a sequence of variables in $x_1,\dots,x_n$ with repetitions and missing variables.

\textbf{proof}: if $\vec{x} = (x_1,\dots, x_n)$,

\begin{equation*}
  g(\vec{x}) = f(\cup_{i1}^n(\vec{x}),\dots,\cup_{ik}^n\vec{x})
\end{equation*}

\fi


\begin{example}
  if $f:\nat^2 \rightarrow \nat $ is computable, then the following are also computable
\begin{itemize}
\item $f_1(x,y) = f(y,x)$;
\item $f_2(x) = f(x,x)$;
\item $f_3(x,y,z) = f(x,y)$.
\end{itemize}
\end{example}


\begin{remark}
  On the basis of this result we can use generalized composition when the $g_i$ are not functions of all the variables or are functions with repetitions.
\end{remark}

\begin{example}
  Given $f: \nat^2 \rightarrow \nat$ where $ f(x_1,x_2) = x_1 + x_2 $ is computable, we can derive that $g: \nat^3 \rightarrow \nat$ where $ g(x_1,x_2,x_3) = x_1 + x_2 + x_3 $ is also computable.
  In fact $g(x_1,x_2,x_3) = f(f(x_1,x_2),x_3) $. Then, we can think about $x_1,x_2,x_3$ as functions $\nat^3\rightarrow\nat$ on $\vec{x}$, so we get to $f(f(U_1^3(\vec{x}),U_2^3(\vec{x})), U_3^3(\vec{x}))$, that is computable.
\end{example}

\begin{example}
  The following functions are computable
\begin{itemize}
\item \textbf{constant} $\lambda \vec{x}.m$, as $m(\vec{x}) = s(s(\dots s(z(\vec{x}))))$, $s$ applied $m$ times;
\item \textbf{addition} $g(x_1,\dots,x_k) = x_1 + \dots + x_k$, seen before;
\item \textbf{product by constant} $g(x,\dots,x) = k \cdot x$, where $g$ is the function at the previous step;
\item if $f(x,y)$ is computable, then also $f'(x) = f(x,m)$ is computable.
  In fact $f'(x) = f(x,m) = f(U_1^1(x)), m(x))$, that is computable;
\item if $f:\nat\rightarrow\nat$ is total computable, the predicate $Q(x,y)\equiv f(x) = y$ is decidable.

  In fact, we know that \begin{equation*}
    \mathcal{X}_{Eq}(x,y) = \begin{cases}
    1 & x=y         \\
    0 & $otherwise$
  \end{cases}
\end{equation*}
  is computable.

  Therefore $\mathcal{X}_Q(x,y) = \mathcal{X}_{Eq}(f(x),y) = \mathcal{X}_{Eq}(f((U_1^2(x,y)), (U_2^2(x,y))$, so $\mathcal{X}_Q$ is computable.
\end{itemize}
\end{example}


\section {Primitive recursion}

\textbf{Recursion} is a familiar concept; it allows to define a function specifying the values in terms of other values of the function itself (and possibly using other already defined functions).

\begin{example}[Factorial]
  \[
    \begin{cases}
      0! = 1\\
      (n+1)! = n! \cdot (n+1)
    \end{cases}
  \]
\end{example}

\begin{example}[Fibonacci]
  \[
    \begin{cases}
      f(0) = 1 \\
      f(1) = 1 \\
      f(n+2) = f(n) + f(n+1)
    \end{cases}
  \]
\end{example}

There are many types of recursion, here we use a ``controlled'' version of recursion.
\begin{definition}[Primitive recursion]
  Given $f:\nat^k\rightarrow\nat$ and $g:\nat^{k+2}\rightarrow\nat$ functions,
we define $h:\nat^{k+1}\rightarrow\nat$ by \textbf{primitive recursion} as follows
\begin{equation*}
  \begin{cases}
    h(\vec{x},0) = f(\vec{x})\\
    h(\vec{x}, y+1) = g(\vec{x},y,h(\vec{x},y))
  \end{cases}
\end{equation*}
\end{definition}

\begin{remark}
  The function $h$ is defined in an equational manner, with $h$ that appears on both sides: 
  it is an implicit definition, not obvious that such $h$ exists or that it is unique, 
  but actually it does exist and it is unique. However, a general theory that supports this observation is not trivial.

  The argument proceeds as follows

\begin{enumerate}
\item let $\nat^n\rightarrow\nat$ the set of functions on natural numbers with $n$ arguments
\item we define an operator
  \begin{align*}
    &T: (\nat^{k+1}\rightarrow\nat) \rightarrow (\nat^{k+1}\rightarrow\nat)\\
    &T(h)(\vec{x},0) = f(\vec{x})\\
    &T(h)(\vec{x},y+1) = g(\vec{x},y,h(\vec{x},y))
  \end{align*}
\item the wanted functions are fixed points of $T$, i.e. $h$ such that $T(h) = h$;
\item the existence of the fixed point follows from these properties
  \begin{itemize}
  \item $\nat^{k+1}\rightarrow\nat$ is a chain partial ordered set;
  \item $T$ is continuous;
  \item Scott functions have a least fixed point.
  \end{itemize}
\item uniqueness follows inductively, so if $h,h'$ fixed points then $h=h'$.
\end{enumerate}
\end{remark}

\begin{example}
  Consider the sum function $ h(x,y) = x+y $ with $ h(x,0) = x = f(x) $ and $ h(x,y+1) = h(x,y) + 1 = g(h(x,y)) $.
  
  $f$ is the identity and $g$ is the successor.
   Both are computable, so the sum is computable by primitive recursion.
\end{example}



\begin{proposition}
  Functions obtained from total functions by
\begin{enumerate}
\item generalized composition
\item primitive recursion
\end{enumerate}
are total.
\begin{proof}
  \begin{enumerate}
    \item obvious by definition;
    \item Let $f:\nat^k\rightarrow\nat, g:\nat^{k+2}\rightarrow\nat$ be total functions and define
    $h$ by primitive recursion.
    
    It can be proved by induction on $y$ that 
    \begin{equation*}
      \forall \vec{x}\in\nat^k  \ (\vec{x},y) \in dom(h)
    \end{equation*}
    \begin{itemize}
      \item $(y=0)$: for all $\vec{x}\in\nat^k$, $h(\vec{x},0) = f(\vec{x})\downarrow$;
      \item $(y\rightarrow y+1)$: for all $\vec{x}\in\nat^k$, $h(\vec{x},y+1) = g(\vec{x},y,h(\vec{x},y))\downarrow$ by inductive hypothesis.
    \end{itemize}
  \end{enumerate}
\end{proof}
\end{proposition}





\begin{example}
  \begin{itemize}
    \item \textbf{sum} $x+y$\\
      $x+0 = x\\
      x+(y+1) = (x+y)+1\\\\
      h(x,0) = x\\
      h(x,y+1) = h(x,y)+1\\\\
      f(x) = x\\
      g(x,y,z) = z+1$
    \item \textbf{product}
      $x\cdot y\\
      x\cdot 0 = 0\\
      x\cdot (y+1) = (x\cdot y)+x\\
      \\
      h(x,0) = 0\\
      h(x,y+1) = h(x,y)+x\\
      \\
      f(x) = 0\\
      g(x,y,z) = z+y$
    \item \textbf{factorial}
      $y!\\
      0! = 1\\
      (y+1)! = y!\cdot (y+1)\\
      \\
      h(0) = 1\\
      h(y+1) = h(y)\cdot(y+1)\\
      \\
      f(0) = 1\\
      g(y,z) = z\cdot (y+1)$
    \end{itemize}
  
\end{example}


\begin{proposition}
  $\mathcal{C}$ is closed under primitive recursion.

\begin{proof}
Let $f:\nat^k\rightarrow\nat$ and 
$g:\nat^{k+2}\rightarrow\nat$ be computable functions.
We want to prove that $h:\nat^{k+1}\rightarrow\nat$ defined through primitive recursion
\begin{equation*}
  \begin{cases}
    h(\vec{x},0) = f(\vec{x})\\
    h(\vec{x}, y+1) = g(\vec{x},y,h(\vec{x},y))
  \end{cases}
\end{equation*}
is computable.

Let $F,G$ programs in standard form for $f,g$. We want a program $H$ for $h$.
We proceed as suggested by the definition.

We start from $\begin{tabu}{|c|c|c|c|c|c|}
  \hline
  x_1 & \dots & x_k & y & 0 & \dots \\
  \hline
\end{tabu}$

we save the parameters and we start to compute $h(\vec{x},0)$ using $F$.

If $y=0$ we are done, otherwise we save $h(\vec{x},0)$ and compute $h(\vec{x},1) = g(\vec{x},0,h(\vec{x},0))$ with $G$. Do the same for $h(\vec{x},i)$ until we arrive to $i=y$

As usual we need registers not used by $F$ and $G$, $m = max\{\rho(F),\rho(G),k+2\}$ and we build the program for $h$ as follows:

$\begin{tabu}{|c|c|c|c|c|c|c|c|c|}
  \hline
  1                     & \dots                                  & m+1                    & \dots   & m+k   & m+k+1 & \dots        & m+k+3 &   \\
  \hline
  \dots                 & \dots                                  & \dots                  & \vec{x} & \dots & i     & h(\vec{x},2) & y     & 0                                                \\
  \hline
\end{tabu}$

$\begin{tabu}{lll}
  & T(1,m+1)                              &                            \\
  & \dots                                 &                           \\
  & T(k,m+k)                              &                            \\
  & T(k+1,m+k+3)                         &                                 \\
  & F[m+1,\dots,m+k\rightarrow m+k+2]    & h(\vec{x},0)                               \\
  LOOP: & J(m+k+1,m+k+3,END)                   & i=y?                                       \\
  & G[m+1,\dots,m+k+2 \rightarrow m+k+2] & h(\vec{x},i+1) = g(\vec{x},i,h(\vec{x},i)) \\
  & S(m+k+1)                             & i = i+1                                    \\
  & J(1,1,LOOP)                          &                                            \\
  END:  & T(m+k+2,1)
\end{tabu}$
\end{proof}
\end{proposition}


\textbf{Note:} We do nothing more than implementing recursion through iteration!

\begin{observation}
  The following functions are computable.
\begin{enumerate}
\item \textbf{sum} $x+y$, see above;
\item \textbf{product} $x \cdot y$ see above;
\item \textbf{exponential} $x^y$\\
  $x^0 = 1, h(x,0) = 1, f(x) = 1$\\
  $x^{y+1} = x^y\cdot x, h(x,y+1) = h(x,y)\cdot x, g(x,y,z) = z\cdot x$;
\item \textbf{predecessor} $x \dot - 1$\\
  $0 \dot -1 = 0, h(0) = 0, f \equiv \underline{0}$\\
  $(x+1)\dotdiv  1 = x, h(x+1) = x, g(y,z) = y$;
\item \textbf{subtraction} $x\dotdiv  y = \begin{cases}
    x-y & x \geq y    \\
    0   & $otherwise$
  \end{cases}$\\
  $x\dotdiv  0 = x, f(x) = x\\
  x\dotdiv (x+1) = (x\dotdiv  y)\dotdiv  1, g(x,y,z) = z\dotdiv  1$;
\item \textbf{sign} $sg(x) = \begin{cases}
    0 & x=0   \\
    1 & x > 0
  \end{cases}\\
  sg(0) = 0, f \equiv \underline{0}\\
  sg(x+1) = 1, g(y,z) = 1$;
\item \textbf{complement sign} $\bar{sg}(x) = \begin{cases}
    0 & x=0 \\
    1 & x>0
  \end{cases}\\
  \bar{sg}(x) = 1 \dotdiv  sg(x), $ composition and (6);
\item $ |x - y| = \begin{cases}
    x-y & x\geq y \\
    y-x & x < y
  \end{cases}$\\
  $ |x - y| = (x\dotdiv y)+(y\dotdiv x)$ from (1), (6) and composition;
\item \textbf{factorial} $y!\\
  0! = 1, f \equiv 1
  (y+1)! = y!(y+1), g(y,z) = (y+1)z $;
\item \textbf{minimum} $min(x,y) = x\dotdiv  (x\dotdiv  y)$;
\item \textbf{maximum} $max(x,y) = (x \dotdiv  y) + y$;
\item \textbf{remainder} $rm(x,y) = \begin{cases}
    y mod y & x \not= 0 \\
    y       & x=0
  \end{cases}$ \\ rest of the integer division of $y$ by $x$ (convention! reasonable if the rest $rm(x,y)$ must be such that $\exists k \mid kx + rm(x,y) = y$)\\
  $rm(x,0) = 0\\
  rm(x,y+1) = \begin{cases}
    rm(x,y)+1 & rm(x,y)+1 \not= x \\
    0         & $otherwise$
  \end{cases}\\
  = (rm(x,y)+1) sg((x\dotdiv  1)\dotdiv  rm(x,y))\\
  f(x) = 0, g(x,y,z) = z * sg(x\dotdiv 1\dotdiv z)$ OK!


  
\item \textbf{quotient} = $qt(x,y) = y$ div $x$, by convention $qt(0,y) = y$\\
  we define:\\
  $qt(x,0) = 0\\
  qt(x,y+1) = \begin{cases}
    qt(x,y)+1 & rm(x,y)+1=x  \\
    qt(x,y)   & $ otherwise$
  \end{cases}\\
  = qt(x,y) + sg((x\dotdiv 1)\dotdiv rm(x,y))$

\item $div(x,y) = \begin{cases}
    1 & x|y                                   \\
    0 & $otherwise, $0|0 $ and $ 0\not|y, y>0
  \end{cases}\\
  div(x,y) = \bar{sg}(rm(x,y))$
\end{enumerate}
\end{observation}

\begin{corollary}[Definition by cases]
  Given $f_1,\dots,f_n: \nat^k \rightarrow \nat$ total, computable and
  $Q_1,\dots,Q_n \subseteq \nat^k$ decidable, predicate and mutually
  exclusive (for each $\vec{x} \in \nat^k$, \textbf{exactly one} of
  $Q_1,\dots,Q_n$ holds) then $ f:\nat^k \rightarrow \nat $ is total
  computable where
  \begin{equation*}
    f(\vec{x}) = \begin{cases}
      f_1(\vec{x}) & Q_1(\vec{x}) \\
      f_2(\vec{x}) & Q_2(\vec{x}) \\
      \dots        &              \\
      f_n(\vec{x}) & Q_n(\vec{x})
    \end{cases}
  \end{equation*}
\end{corollary}

\begin{proof}
$f(\vec{x}) = f_1(\vec{x}) \cdot \mathcal{X}_{Q1}(\vec{x}) + \dots + f_n(\vec{x}) \cdot \mathcal{X}_{Qn}(\vec{x})$

We conclude using the computability of sum and product and the fact that composition preserves computability.
\end{proof}

\section{Algebra of decidability}

\begin{lemma}
  Let $Q, Q'$ be  decidable predicates.  Then also $\neg Q, Q \wedge Q', Q \vee Q'$ are decidable.
\end{lemma}

\begin{proof}
It is enough to observe that:
\begin{enumerate}
\item $\mathcal{X}_{\lnot Q}(\vec{x}) =  \overline{sg}(\mathcal{X}_Q(\vec{x}))$
\item $\mathcal{X}_{Q \vee Q'}(\vec{x}) = \mathcal{X}_{Q}(\vec{x}) \cdot \mathcal{X}_{Q'}(\vec{x})$
\item observe that $Q \wedge Q' = \lnot (\lnot Q \vee \lnot Q')$
\end{enumerate}
\end{proof}

We remind that $\{\neg, \wedge, \vee \}$ ($\{\neg, \vee \}$ is enough) is a functionally complete set of connectives (it allows to express any function $\{0,1\}^n \rightarrow \{0,1\}$). We deduce that:
\begin{corollary}
  Let $Q_1, \dots, Q_n \subseteq \nat^k$ decidable predicates and let $f:\{0,1\}^n \rightarrow \{0,1\}$ a function, let us consider:
  \begin{quote}
    $\mathcal{X}: \nat^k\rightarrow\{0,1\}$\\
    $\mathcal{X}(\vec{x}) = f(\mathcal{X}_{Q_1}(\vec{x}), \dots, \mathcal{X}_{Q_n}(\vec{x}) )$
  \end{quote}
  Then the predicate $Q$ which corresponds to $\mathcal{X}$ is
  decidable, and therefore $\mathcal{X}$ is computable.
\end{corollary}

\section{Sum, product, bounded quantification}

\begin{definition}[Bounded sum and product]
  Let $f:\nat^{k+1}\rightarrow\nat$ be a total function. Then
  
  \begin{itemize}
    
  \item 
    $\sum_{z<y}f(\vec{x},z)$ is defined by
    \begin{align*}
      \sum_{z<0}f(\vec{x},z) &= 0 \\
      \sum_{z<y+1}f(\vec{x},z) &= \sum_{z<y}f(\vec{x},z) + f(\vec{x},y)
    \end{align*}
    
    
  \item $\prod_{z<y}f(\vec{x},z)$ is defined by:
    \begin{align*}
      \prod_{z<1}f(\vec{x},z) &= 1 \\
      \prod_{z<y+1}f(\vec{x},z) &= \prod_{z<y}f(\vec{x},z) \cdot f(\vec{x},y)
    \end{align*}
  \end{itemize}
\end{definition}

\begin{lemma}
  If $f:\nat^{k+1}\rightarrow\nat$ is total computable then
  \begin{enumerate}
  \item $g(\vec{x},y) = \sum_{z<y}f(\vec{x},y)$
  \item $h(\vec{x},y) = \prod_{z<y}f(\vec{x},y)$
  \end{enumerate}
  are total computable.
\end{lemma}

\begin{proof}
  Just note that they are defined by primitive recursion!

  \begin{quote}
    $g(\vec{x},0) = 0$\\
    $g(\vec{x},y+1) = g(\vec{x},y) + f(\vec{x},y)$
  \end{quote}
  
  and $+,f$ are computable.

  Same for 2.
\end{proof}

Obviously, by closure under composition, the bound can be a total computable function.

Another immediate consequence concerns the decidability of the bounded quantification on the predicates.

\begin{lemma}
  Let $Q\subseteq \nat^{k+1}$ be a decidable predicate, then:
  \begin{enumerate}
  \item $Q_1(\vec{x},y) \equiv \forall z<y. Q(\vec{x},z)$
  \item $Q_2(\vec{x},y) \equiv \exists z<y. Q(\vec{x},z)$
  \end{enumerate}  
  are decidable.
\end{lemma}

\begin{proof}
  \begin{enumerate}
  \item observe that $\mathcal{X}_{Q_1}(\vec{x},y) = \prod_{z<y}\mathcal{X}_Q(\vec{x},z)$
  \item observe that $\mathcal{X}_{Q_2}(\vec{x},y) = sg(\sum_{z<y}\mathcal{X}_Q(\vec{x},z))$
  \end{enumerate}
\end{proof}

\section{Bounded minimalisation}
Given a function $f: \nat^{k+1} \rightarrow \nat$, we define a function $h: \nat^{k+1} \rightarrow \nat$ as follows:

\begin{equation*}
  h(\vec{x},y) = \mu z<y . f(\vec{x},z) =
  \begin{cases}
    $min.$z<y$ such that $ f(\vec{x},z) = 0 & $ if it exists$ \\
    y                                   & $ otherwise $
  \end{cases}
\end{equation*}

%(query-replace-regexp "\\$ \\([^\n$]*\\\\[^\n$]*\\) \\$" "$\\1$")

\begin{lemma}
  Let $f: \nat^{k+1} \rightarrow \nat$ total computable. Then also
  $h: \nat^{k} \rightarrow \nat$ defined by
  $h(\vec{x},y) = \mu z<y. f(\vec{x},z)$ is (total) computable.
\end{lemma}

\begin{proof}
  We observe that $h$ can be defined as:
  
  \begin{quote}
    $h(\vec{x},y) = \sum_{z<y}\prod_{w\leq z} sg(f(\vec{x},w))$
  \end{quote}
  
  The product value is $1$ on the intervals $[0,z]$ in which $f\not= 0$,
  i.e. if $z_0$ is the min $z<y$ where $f$ is null, they're equal to
  $z_0$, therefore the external sum counts them.

  Alternatively $h$ can be defined directly through primitive recursion:
  \[
  \begin{cases}
    h(\vec{x},0) = 0 \\
      h (\vec{x},y+1)
      & =
        \begin{cases}
          h(\vec{x},y)               & h(\vec{x},y)\not= y \\
          \begin{cases}
            y   & f(\vec{x},y) = 0 \\
            y+1 & $otherwise$
          \end{cases} & $ otherwise $
        \end{cases}\\[2mm]
        % 
      & =sg(y-h(\vec{x},y)) \cdot h(\vec{x},y) + \bar{sg}(y-h(\vec{x},y))(y+sg(f(\vec{x},y)))
  \end{cases}
  \]
\end{proof}

\begin{lemma}
  The following functions are computable:
\begin{enumerate}[label=\alph*)]
\item $D(x) = $ number of divisors of $x$
\item $Pr(x) = \begin{cases}
    1 & $ x is prime $ \\
    0 & $ otherwise $
  \end{cases}$ (x prime is decidable)
\item $p_x$ = $x$-th prime number (convention: $p_0=0, p_1=2,p_2=3\dots$)
\item $(x)_y = \begin{cases}
    $exponent of $p_y$ in the factorization of $x & x,y > 0      \\
    0                                             & x=0 \vee y=0
  \end{cases}$\\
  e.g. $72 = 2^3\cdot 3^2, (72)_1 = 3, (72)_2 = 2, (72)_3 = 0$
\end{enumerate}
\begin{proof}
\begin{enumerate}[label=\alph*)]
\item $D(x) = \sum_{y\leq x}div(y,x)$
\item $Pr(x)$ is $1$ if $x>1$ and is divided only by 1 and itself
      \begin{align*}
        Pr(x) &= \begin{cases}
          1 & D(x) = 2      \\
          0 & $ otherwise $
        \end{cases} \\
        &= \bar{sg}(|D(x)-2|)
      \end{align*}    

\item $P_x$ can be defined by primitive recursion
  \begin{align*}
    &P_0=0 \\
    &P_{x+1} = \mu z \leq (P_x!+1) . \bar{sg}(P_z(z)\cdot \mathcal{X}_{z>Px}(z))
  \end{align*}
  Certainly $P_{x+1} \leq P_x!+1$, in fact, 
  call $p$ a prime in the decomposition of $p_x!+1$, therefore $p\mid p_x!+1$, so $p>p_x$, otherwise $p \mid p_x!$ and therefore $p \mid 1$. 
  Thus $p_x < p_{x+1} \leq p$.

\item Note that
  \begin{align*}
       (x)_y & = \max \ z . p_y^z \mid x = \\
             & = \min  \  z . p_y^{z+2}\not \mid x\\
             & = \mu z\leq x . \lnot div((p_y)^{z+1},x)
  \end{align*}
\end{enumerate}
\end{proof}
\end{lemma}

\subsection{Exercises}
Prove that the following functions are computable:

\begin{enumerate}[label=\alph*)]
\item $\floor{\sqrt{x}}$

  $\floor{\sqrt{x}} = max\, y\leq x \quad y^2 \leq x\\
  = min \, y \leq x \quad (y+1)^2 > x\\
  \mu y\leq x. ((x+1)-(y+1)^2)$
\item $\mathit{lcm}(x,y)\\
  mxm(x,y) = \mu < \leq x\cdot y . (x|z $ and $ y|z)\\
  = \mu z \leq x\cdot y. \bar{sg}(dic(x,z)\cdot div(y,z))$
\item $\mathit{GCD}(x,y)$

  Certainly $\mathit{GCD}(x,y)\leq min\{x,y\}$ and it can be characterized using the minimum number that can be subtracted to $min\{x,y\}$ to obtain the divisor of $x,y$

  $\mathit{GCD}(x,y)\leq min(x,y)-\mu z\leq min(x,y).(1\dotdiv div(min(x,y)-z,x)\cdot div(min(x,y)-z, y))$
\item number of prime divisors of $x$

  $\sum_{z\leq x} pr(z)\cdot div(z,x)$
\end{enumerate}

\section{Encoding of pairs (and n-tuples)}

Let us see an encoding in $\nat$ of pairs (and n-tuples) of natural numbers that will later be proved useful for some considerations on recursion (and for the future\dots)

Let us define as a \textbf{pair encoding}:
\begin{quote}
  $\pi: \nat^2\rightarrow\nat$\\  
  $\pi(x,y) = 2^x(2y+1)-1$
\end{quote}

Notice that $\pi$ is bijective and effective (computable).

The inverse can be characterized in terms of two computable functions that give the first and second component of a natural $x$ seen as pair:

\begin{quote}
  $\pi^{-1}:\nat\rightarrow\nat^2$\\
  $\pi^{-1}(x) = (\pi_1(x),\pi_2(x))$
\end{quote}
%
where $\pi_1(x) = (x+1)_1$ and 
$\pi_2(x) = (\frac{x+1}{2\pi_1(x)}-1)/2$.

(the division is $qt(\_,\_)$)

It can be generalized to an encoding of $n$-tuples:
\begin{quote}
  $\pi^n: \nat^n\rightarrow\nat \quad n\geq2$
\end{quote}
defining inductively
\begin{quote}
  $\pi^2 = \pi$\\
  $\pi^{n+1}(\vec{x},y) = \pi(\pi^n(\vec{x},y)) \quad \vec{x} \in \nat^n, y \in \nat$
\end{quote}

and correspondingly we can define the projections $pi_j^n:\nat\rightarrow\nat^n$

\subsection{Considerations on recursion}

The Fibonacci function is defined by:

\begin{quote}
  $ fib(0) = fib(1) = 1$\\
  $fib(n+2) = fib(n) + fib(n+1)$
\end{quote}

This is not exactly a definition by primitive recursion. Given that $f(y+2)$ is defined in terms of $f(y)$ and $f(y+1)$, it does not respect the schema \dots

We can show that $f$ is computable by resorting to the encoding and defining:

\begin{quote}
  $g:\nat\rightarrow\nat$\\
  $g(y) = \pi(f(y),f(y+1))$\\
\end{quote}

therefore $g$ can be defined by primitive recursion:

\begin{quote}
  $\begin{array}{ll}
     g(0) & =  \pi(f(0),f(1)) = \pi(1,1)\\[2mm]
     %
     g(y+1)  & = \pi(f(y+1),f(y+2)) = \pi(f(y+1),f(y)+f(y+1))\\
              & = \pi(\pi_2(g(y)), \pi_1(g(y)) + \pi_2(g(y)))
   \end{array}
   $
\end{quote}
so $g$ is computable, by primitive recursion.

Finally, $f(y) = \pi_1(g(y))$ is computable by composition.

\medskip

In general we could have a function $f$ defined using $k$ previous values

$\begin{cases}
  f(0) = c_0   \\
  f(k-1) = c_k \\
  f(y+k) = h(f(y),\dots,f(y+k-1))
\end{cases}$

with $h:\nat^k\rightarrow\nat$ computable.

One can proceed like before and define

\begin{quote}
  $g:\nat\rightarrow\nat$\\
  $g(y) = \pi^k(f(y),\dots,f(y+k-1))$
\end{quote}

Then function $g$ can be defined by primitive recursion

\begin{quote}
  $g(0) = \pi^k(c_0,\dots,c_{k-1})$\\
  $g(y+1) = \pi^k(f(y+1),\dots,f(y+k-1),f(y+k))$
\end{quote}
where
\begin{quote}
  $\begin{array}{ll}
     f(y+1) & = \pi_2^k(g(y))\\
     f(y+k-1) & = \pi_k^k(g(y))\\
     f(y+k) & = h(f(y),\dots,f(y+k-1)) \\
            & = h(\pi_1^k(g(y)),\dots,\pi_k^k(g(y)))\\
            & = \pi^k(\pi_2^k(g(y)),\dots,\pi_k^k(g(y)),h(\pi_1^k(g(y)),\dots,\pi_k^k(g(y))))
   \end{array}
   $
 \end{quote}

 g is computable, so $f(y) = \pi_1(g(y))$ is computable.

\section{Unbounded minimalisation}
The operators to manipulate functions seen until now, generalized composition and primitive recursion, starting from \textbf{total} functions return total functions. 
Another essential operator, which allows to build partial functions is the \textbf{unbounded minimalisation} operator.

Similar to the bounded minimalisation, given $f(\vec{x},y)$ not necessarily total, it defines, informally, the following function:
\[
  \mu y . f(\vec{x},y)  = \mbox{ minimum } y \mbox{ s.t. } f(\vec{x},y) = 0.
\]
But there are two cases in which we have problems
\begin{enumerate}
\item if there is no $y$ s.t. $f(\vec{x},y) = 0 \uparrow$
\item if before finding a $y$ s.t. $f(\vec{x},y) = 0$, it happens that $f(\vec{x},z)\uparrow$
\end{enumerate}
This is intuitive if we think about the obvious algorithm to compute the minimalisation: start from 0, $f(\vec{x},0) = 0$? if yes then $out(0)$, otherwise $f(\vec{x},1) = 0$? until $f(\vec{x},y) = 0$.

\begin{definition}
  Let $f : \nat^{k+1}\rightarrow\nat$ be a function. Then the function
  $h:\nat^k\rightarrow\nat$ defined through \textbf{unbounded
    minimalisation} is:

  \begin{equation*}
    h(\vec{x}) = \mu y. f(\vec{x},y) = \begin{cases}
      $least $ z$ s.t. $ & \begin{cases}
        f(\vec{x},z) = 0 \\
        f(\vec{x},z)\downarrow \quad f(\vec{x},z') \not= 0 \quad $ for $ z<z'
      \end{cases} \\
      \uparrow           & $ otherwise $
    \end{cases}
  \end{equation*}
\end{definition}

\section{Closure under minimalisation}

\begin{theorem}
  Let $f:\nat^{k+1}\rightarrow\nat$ a computable function (not necessarily total). Then $h:\nat^k\rightarrow\nat$ defined by $h(\vec{x}) = \mu y. f(\vec{x},y)$ is computable.
\end{theorem}

\begin{proof}
  Let $F$ be a program in standard form for $f$.

  \textbf{idea:} for $z=0,1,2,\dots$ we compute $f(\vec{x},z)$ until we find a zero\dots
  
  We need to save the argument $\vec{x}$ in a zone that is not used by the program $F$.

$m = max\{\rho(F),k+1\}$

So the program for $h$ is obtained as follows:

$\begin{tabu}{cccccccc}
  1                            & \dots                  & k                             & \dots                  & m+1 & \dots & m+k & m+k+1 \\
  \hline
  \multicolumn{3}{|c}{\vec{x}} & \multicolumn{1}{|c|}{} & \multicolumn{3}{|c|}{\vec{x}} & \multicolumn{1}{c|}{z}                             \\
  \hline
\end{tabu}$

$\begin{tabu}{lll}
  & T([1,k],[m+1,m+k])
  & \mbox{saves $\vec{x}$}\\
  %
  LOOP: & F[m+1,\dots,m+k+1\rightarrow 1] & f(\vec{x},z) \rightarrow R_1                        \\
  & J(1,m+k+2,END)                  &  \mbox{$f(\vec{x},z) = 0$? (recall that $m+k+z$ contains $0$)}\\
  & S(m+k+1)                        & z=z+1                                               \\
  & J(1,1,LOOP)                     &                                                     \\
  END:  & T(m+k+1,1)
\end{tabu}$
\end{proof}

\textbf{Note}: Observe that $F$ may not terminate \dots this is correct! The entire program does not terminate and $\mu$ is undefined!

\textbf{Note:} This is nothing more than a \textbf{while} loop implemented with \textbf{goto}.

\textbf{Observation}: The $\mu$ operator allows us to obtain \textbf{non total} functions starting from total functions.

\begin{example}
  Given $f(x,y) = |x-y^2|$, we have that
  \begin{quote}
    $\mu y. f(x,y) =
    \begin{cases}
      \sqrt{x} & x $ is a perfect square $ \\
      \uparrow & $ otherwise $
    \end{cases}$
  \end{quote}
\end{example}

\begin{exercise}
  Let $f:\nat\rightarrow\nat$ be computable, total and injective. The
  the \textbf{inverse} $f^{-1} = \begin{cases}
    y        & f(y) = x                \\
    \uparrow & \not\exists y. f(y) = x
\end{cases}$

is computable. In fact, in our hypothesis' $f^{-1}(x) = \mu y. |f(y)-x|$
\end{exercise}

\textbf{Note} The above proof uses in an essential way the fact that $f$ is total, but the result is completely general \dots

Intuitively, when $f$ is not total, to find $f^{-1}(x)$ we consider a program $P$ for $f$ and I execute it as follows:
\begin{itemize}
\item 0 steps of the program on argument 0
\item 1 step on 0
\item 0 steps on 1
\item 2 steps on 0\\
  \dots
\end{itemize}

in a dove tail execution pattern.

Every time for a certain number of steps $k$ on argument $y$, the program \textbf{terminates} we check the output $f(y)$, if $f(y) = x$ we stop, otherwise we continue.

Informal \dots we will see how to formalize it.

\begin{exercise}
  Prove that the following function is computable.
  \[f(x,y) = \begin{cases}
    \frac{x}{y} & y\not= 0 \land y|z \\
    \uparrow    & $ otherwise $
  \end{cases}\]
  \begin{proof}
    \[ f(x,y) = \mu z. (|yz-x| + \mathcal{X}_{x=0\land y=0}(x,y)) \]
  \end{proof}
\end{exercise}


\begin{lemma}
  All the functions with finite domain are computable, i.e. let
  $\theta: \nat\rightarrow\nat$ with $dom(\theta)$ finite, then
  $\theta$ is computable.
\end{lemma}
  
\begin{proof}
  Let $\theta:\nat\rightarrow\nat$ a finite domain function
  \[
    \theta=\{(x_1,y_1),\dots,(x_n,y_n)\}
  \]
  i.e.
  
  \[
    \theta(x) = \begin{cases}
      y_1      & x=x_1         \\
      \dots                    \\
      y_n      & x=x_n         \\
      \uparrow & $ otherwise $
    \end{cases}
  \]
  then
  \[
    \theta(x) = \sum_{i=1}^{n}y_i \cdot \bar{sg}(|x-x_i) + \mu z. (\prod_{i=1}^{n}|x-x_i|)
  \]
  The minimalisation is needed only to make the function $\uparrow$ when $x\not= x_1,\dots,x_n$, it is $0$ otherwise.
\end{proof}