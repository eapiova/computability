\chapter{Pimitive recursive functions}
Let's consider again the class of primitivi recursive functions.

\textbf{Definition} (primitive recursive funcitons).  The class of
primitive recursive functions is the smallest set
$\mathcal{PR} \subseteq \bigcup\limits_k(\mathbb{N}^k \rightarrow
\mathbb{N})$ containing.

\begin{enumerate}[label=(\alph*)]
\item zero
\item successor
\item projections
\end{enumerate}

and closed under

\begin{enumerate}[label=(\arabic*)]
\item composition
\item primitive recursion
\end{enumerate}

There are many interesting point of $\mathcal{PR}$. For example the
fact that primitive recursion are similar to \texttt{for} loops, while
minimalization is similar to \texttt{while} loops. This fact can be
formalized into a new variant on the URM, with \textit{structured
  programs}, where the jump instruction is substituted with
\texttt{for} and \texttt{while} loops. We'll call this machine
$\text{URM}_{\text{for,while}}$ (This corresponds to consider the
subset of URM programs where the jump instructions are used in a
``disciplinated'' way).

We can deonstrate that this model has the same expressive power of the
URM model. This means that the class $\mathcal{C}_{\text{for,while}}$
fo functions computable in this model is

\[
  \mathcal{C}_{\text{for,while}} = \mathcal{C} = \mathcal{R}
\]

while the class of functions computable using only the \texttt{for}
construct are partial recursive functions:

\[
  \mathcal{C}_{\text{for}} = \mathcal{PR}
\]
So studing the relation between $\mathcal{R}$ and $\mathcal{PR}$ is
the same as studying the relation between the expressive power of
\texttt{for} and \texttt{while} constructs.  We know that many
``arithmetic'' functions, like Pr$(x), (x)_y,$ qt, mcm($x,y$),
$x^y, \dots$ are in $\mathcal{PR}$ and that it is closed for sum,
product and minimalization (all of them limited). This class is very
extended, but it does not contain all computable functions, in oter
words $\mathcal{PR} \subsetneq \mathcal{R}$, because $\mathcal{PR}$
functions are always total (based on an inductive caratheristic,
$\mathcal{PR}$ functions are obtainable for base total functions by
composition and primitive recursion).

One could think that $\mathcal{PR}$ includes all total reursive
functions, in other words if Tot is the set of all total functions:
$\mathcal{PR} = \mathcal{R} \cap \text{Tot}$ [Hilbert, 1926].

This is also false
($\mathcal{PR} \subsetneq \mathcal{R} \cap \text{Tot}$). In fact even
if we restrain ourselves to total functions (programs that always
terminates) the \texttt{while} construct is essential.

\section{Ackermann function}
$ \psi: \nat^2 \rightarrow \nat $ defined as:

$ \psi(0,y) = y+1 $

$ \psi(x+1,0) = \psi(x,1) $

$ \psi(x+1,y+1) = \psi(x, \psi(x+1, y)) $

Formally, $\psi$ is the minimum fixed point of the operator associated
to the recursive function, on the cpo % not sure about this
of partial functions
$T: (\mathbb{N}^2 \rightarrow \mathbb{N}) \rightarrow (\mathbb{N}^2
\rightarrow \mathbb{N})$. More intuitively, the scheme univocally
guesses a function, because the value $\psi(x,y)$ is always defined
based on ``smaller'' values of $\psi$ itself. But what does it mean
``smaller''?

$\psi(x+1,0) = \psi(x,1)$; here the first component
diminishes. $\psi(x+1,y+1)=$ at first we calculate $\psi(x+1, y)$,
here the second component diminishes, and then $\psi(x,u)$. Whatever
$u$ is, the first component is smaller.

We can see that the arguments diminishes in a \emph{lexicographical}
order on $\mathbb{N}^2$

with $(\mathbb{N}^2, \leq_{lex} )$, we have that $(x,y) \leq (x', y')$
if $(x < x') \wedge (x=x'$ and $y \leq y')$, and so
$( \mathbb{N}^2, \leq_{lex} )$ is \emph{well ordered}, in the sense
that it does not allow for infinite descending sequences.

\paragraph{Parenthesis on partial orders}
\begin{definition}
  Let $(D, \leq)$ be a partial order. It is
  \begin{description}
  \item[well ordered] if each non-void subset has the minimum element:
    $$\forall X \subseteq D, X \neq \emptyset \quad \exists \min D$$
  \item[well based] if each non-void subset has a minimal element
    $$ \forall X \subseteq D, X \neq \emptyset \Rightarrow \exists d \in X \text{ minimal}$$
  \end{description}
\end{definition}

\textbf{Observation:} $D$ is well formed $\Rightarrow$ $D$ is well
based.

\textbf{Observation:} if $D$ is well based it does not allow for
infinite descending sequences:
\[
  d_0 > d_1 > d_2 > \dots > d_n > d_n+1 \dots
\]
\newcommand{\conf}{\text{conf}} This fact can be useful when dealing
with termination problems. If we can conclude that the set of
configurations is well based, to end our proof we just need to prove
that for each step \( \conf _i \rightarrow \conf_{i+1} \) and
\( \conf _{i+1} < \conf _i \). This way our computation follows a descending sequence of values, necessarely finite.

In fact, if we think the calculation of $\psi$ is based on the
calculation of $\psi$ with smaller values necessarely at some point it
will reach for sure the case $\psi(0,y) = y + 1$, terminating.

\textbf{Note:} $(\nat^2, \leq_{lex})$ is \emph{well ordered}

(given $\emptyset \neq X \subseteq \nat^2$ we can identify
$x_0 = \min\{x \; | \; \exists y.(x,y) \in X\}$ and
$y_0 = \min \{ y \; | \; (x_0,y) \in X\}$ we can say that
$\min X = (x_0, y_0)$. In this way we can prove that the product of
two well ordered sets is ordered.)

The order is also total (\emph{left as an exercise})
\begin{itemize}
\item ``infinitely many copies of $\nat$, one after the other''
\item eache element has a successor, but not always a predecessor
\item following the path back we have jumps of infinite length
\end{itemize}

Associated with a well based order there is an induction principle:

\paragraph{\textbf{Complete (strong) induction}}

Let $(D, \leq)$ be a well based order, and let $P \subseteq D$ a
property. If
\[
  \forall d \in D \; . \; (\forall d' < d \quad P(d)) \Rightarrow P(d') \Rightarrow P(d)
\]

To prove that $P$ is valid on $D$ we have to prove that
\begin{itemize}
\item is valid on minimal elements
\item for each other element $d$, if it holds on elements smaller than
  $d$, then it holds on $d$
\end{itemize}

The proof is left as an exercise.

\paragraph{\textbf{Back to the Ackerman function}}

We will see that $\psi$:
\begin{enumerate}
\item It is total;
\item $ \psi \in \mathcal{C} = \mathcal{R} $
\item $ \psi \not \in \mathcal{PR} $
\end{enumerate}

% This part was not on the original notes but the prof added it so im
% keeping it

% We prove by induction on pairs $ (x',y') \leq_{lex} (x,y) $
% lexicographically ordered.

% \begin{enumerate}
% \item We proceed by case. If x$ = 0$ then $y + 1$ holds then the function is defined;
% \item $ x>0, y=0 $, then it holds $ \psi(x-1,1) $ which is def. by inductive hypothesis;
% \item $ x>0, y>0 $, then $ \psi(x, \psi(x+1, y)) $ defined by inductive hypothesis, both inside and outside;
% \end{enumerate}

% So the function is total.

\newcommand{\nsqlex}{\( (\nat ^2, \leq_{lex})\) }
\begin{theorem}[$\psi$ is total]
  \[\forall (x,y) \in \nat^2 \quad \psi (x,y) \downarrow \]
  \begin{proof}
    We can prove it with complete induction on \nsqlex:

    \begin{itemize}
    \item \textbf{base elements}. The truth is that \nsqlex has just
      one base element $(0,0)$ and $\psi(0,0)=1\downarrow$
    \item \textbf{inductive step}.
      \[\forall (x,y) \in \nat^2 ((\forall (x', y') \in \nat^2 \;
        (x',y') \leq_{lex} (x,y) \; \psi(x,y)\downarrow ) \Rightarrow
        \psi(x,y)\downarrow)\] we have then 2 subcases:
      \begin{enumerate}
      \item[$(x=0)$] \(\psi(0,y) = y + 1 \downarrow\)
      \item[$(x>0)$] again two subcases
        \begin{enumerate}
        \item[$(y=0)$] $\psi(x,0) = \psi(x-1,1) \downarrow $ for
          inductive hypothesis, since $(x-1,1) \leq_{lex} (x,0)$
        \item[$(y>0)$] $\psi(x,y) = \psi(x-1,\psi(x,y-1)) \downarrow $
          and $\psi(x,y-1) \downarrow$ for inductive hypothesis. If we
          call $u = \psi(x,y-1)$ we have that
          $\psi(x,y) = \psi(x-1,u) \downarrow $ for inductive
          hypothesis
        \end{enumerate}
      \end{enumerate}
    \end{itemize}
  \end{proof}
\end{theorem}

\begin{exercise}
  Given a box with an arbitrary number of balls in it, each one with a
  number in $\nat$ do the following:
  \begin{itemize}
  \item extract a ball
  \item substitute the extracted ball with an arbitrary number of
    balls, each one with a label lower than the extracted one.
  \end{itemize}

  Prove that this process always terminates.
\end{exercise}

\begin{theorem}[$\psi$ is computable]
  \[\psi \in \mathcal{C} = \mathcal{R}\]
  \begin{proof}
    We can use the Church-Turing thesis to prove it: the computation
    of $\psi(x,y)$ is always reduced to the computation of $\psi$ on
    smaller input values, and for the base case ...

    To do so, we'll need the definition of a valid set. Intuitively a
    set $S \subseteq \nat$ is valid if

    \begin{itemize}
    \item \((x,y,z) \in S \quad \Rightarrow \quad z = \psi(x,y)\)
    \item \((x,y,z) \in S \quad \Rightarrow \quad S\) contains all the
      tuples \((x', y', \psi(x',y'))\) needed in order to calculate
      $\psi(x,y)$
    \end{itemize}

    \textbf{example:}
    $\psi(1,1) = \psi(0,\psi(1,0)) = \psi(0,\psi(0,1)) = \psi(0,2) = 3$

    $\Rightarrow S = {(1,1,3), (0,2,3), (1,0,2), (0,1,2)}$

    Formally:

    \begin{definition}[valid $S \subseteq \nat^3$]
      Let $S$ be a set of tuples such that $S \subseteq \nat^3$. We say
      that $S$ is \emph{valid} if:
      \begin{itemize}
      \item \((0,y,z) \in S \quad \Rightarrow \quad z = y+1\)
      \item \((x+1,0,z) \in S \quad \Rightarrow \quad (x,1,z) \in S\)
      \item
        \((x+1,y+1,z) \in S \quad \Rightarrow \quad \exists u . (x+1,
        y, u) \in S \wedge (x,u,z) \in S \)
      \end{itemize}
    \end{definition}

    We can prove that
    \(\psi(x,y) = z \Leftrightarrow \exists S \subseteq \nat^3\) valid
    and finite s.t. \((x,y,z) \in S\) by complete induction on
    $(x,y)$, leveraging the fact that the validity of a set is
    preserved under union (left as an exercise).

    \newcommand{\vnu}{\text{Val}(\nu)}

    A tuple $(x,y,z)$ can be encoded with an integer using the
    encoding function
    \[\Pi^3 \, : \, \nat^3 \rightarrow \nat \quad \quad (\Pi_i^3 \, : \,
      \nat \rightarrow \nat \text{ are the projections})\]. This way a
    set of tuples becomes a set of natuarl numbers
    $\{x_1, \dots, x_n\}$ that we can encode injectively:
    \[\{x_1, \dots, x_n\} \mapsto P_{x_1}, \dots, P_{x_n}\] Now we
    have $\nu$ the number that rapresents the set of tuples
    $S_\nu$. We can verify that
    \[(x,y,z) \in S_\nu \Longleftrightarrow P_{\Pi(x,y,z)} \; | \;
      \nu \] and the predicate $\vnu=$``$\nu$ encodes a
    set of valid tuples'' is decidable.

    In fact $\vnu$ is true if and only if:
    \begin{itemize}
    \item \(\forall i \leq \nu \quad (\nu)_i = 1\)
    \item{
        \( \forall \omega \leq \nu \quad P_{\omega \; | \; \nu}\) \\
        \[\Rightarrow \begin{cases}
            \Pi_1(\omega) = 0 \quad & \Rightarrow
            \quad \Pi_3(\omega) = \Pi_2(\omega) + 1 \\
            \Pi_1(\omega) > 0 \quad & \Rightarrow \quad
            \begin{cases}
              \Pi_2(\omega) = 0 & \Rightarrow
              \Pi(\Pi_1(\omega),0,\Pi_3(\omega)) \in S_\nu \\
              \Pi_2(\omega) > 0 & \Rightarrow
              \exists u \leq \omega \text{ s.t. } \\
              & \quad \Pi(\Pi_1(\omega), \Pi_2(\omega)-1,u) \in S_\omega \\
              & \quad \Pi(\Pi_1(\omega)-1, u,z) \in S_\omega 
            \end{cases}
            \\
          \end{cases}
        \]
      }
    \end{itemize}

    And the associated caratheristic function
    \[\chi_{\text{Val}} \in \mathcal{PR}\]

    We can also verify that:

    \[
      R(x,y,z) = \begin{cases}

        \chi_{\text{Val}(\omega)} & \text{if $\omega$ encodes for some
          valid $S$ that contains $(x,y,z)$ for some $z$} \\

        0 & \text{otherwise}
        
      \end{cases}
    \]

    \[
      R(x,y,z) = \chi_{\text{Val}(\omega)} \cdot \text{sg} (\omega + 1
      \dot{-} \mu z \leq \omega . \text{div}(P_{\Pi(x,y,z)}, \omega)
    \]

    this way we can write the Ackermann function as

    \[
      \psi(x,y) = \mu (z,y) \; \cdot \; \overline{\text{sg}}(R(x,y,\omega)
      \cdot \text{div}(P_{\Pi(x,y,z)}, \omega))
    \]

    So since it is computable,

    \[
      \psi \in \mathcal{R} = \mathcal{C}
    \]

  \end{proof}
\end{theorem}

\begin{theorem}
  \[ \psi \notin \mathcal{PR} \]
  \begin{proof}
    The fact that the definition of $\psi$ includes a recursion (and
    does not respect the scheme of primitive recursive functions for
    this reason) does not allow us to end our proof immediatly.

    The proof of the fact that $\psi$ is not a primitive recursive
    function is done by showing that $\psi$ ``grows'' faster than
    every function in $\mathcal{PR}$. We altread saw how
    \begin{itemize}
    \item we obtain the sum from the successor
    \item we obtain the product from the sum
    \item we obtain the exponential from the product
    \end{itemize}
    each one for primitive recursion, nested each time more.

    The idea of the Ackermann function is that to push to its limit
    this procedure and capture each possible level in a unique
    function that won't be possible to compute with a finite number of
    nested primitive recursions.

    In fact, by calling \[\psi_x(y) = \psi(x,y)\] we have
    that
    \[
      \psi_{x+1}(y) = \psi_x(\psi_{x+1}(y-1)) =
      \psi^2_{x}(\psi_{x+1}(y-2)) = \dots = \psi_x^{y+1}(1)
    \]

    \(\psi_0(y) = y+1 = \) succ$(x)$

    \(\psi_1(y) = \psi_0^{y+1}(1) = \text{ succ}^{y+1}(1) = y+2\)
    
    \(\psi_2(y) = \psi_1^{y+1}(1) = 2y+3\)

    \(\psi_3(y) = \psi_2^{y+1}(1) = 2^{y+3}-3\)

    e.g.

    \(\psi_0(1) = 2, \quad \psi_1(1) = 3, \quad \psi_2(1) = 5, \quad
    \psi_3(1) = 13, \quad \dots\)

    Intuitively, if x grows in $\psi_x$ so does the level of nesting
    in the functions, wich is equivalent to say that we need more and
    more \texttt{for} loops, and since $x$ can grow to infinity and we
    can't keep nesting \texttt{for} loops to infinity, we need a
    \texttt{while} loop. More precisely we can show that for each
    function $f \in \mathcal{PR}$ if we take $j$ the maximum number of
    nested \texttt{for} cycles, holds asymptotically that
    \[
      P(x_1, \dots, x_k)\downarrow \quad \text{with anumber of steps }
      < \psi_{j+1}(\max\{x_1, \dots, x_k\})
    \]

    This way we see that $\psi_j$ gives a bound to the time complexity
    as well to the number of increment operations, so
    \[f(\vec{x}) < \psi_{j+1}(\max(\vec{x})\]

    But for absurd, if $\psi \in \mathcal{PR}$, we can call $j$ the
    maximum number of nested \texttt{for} cycles in a program that
    computes it, and so \[\psi(x,y) < \psi_{j+1}(\max\{x,y\})\] should
    hold. In particular with $x=y=k>j+1$ big enough we have
    that \[\psi(k,k) < \psi_{j+1}(k) < \psi_k(k) = \psi(k,k)\] wich is
    absurd.
  \end{proof}
\end{theorem}

\newpage

\textbf{Observation:} initially GÃ¶del and Kleene had studied a class
of functions $\mathcal{R}_0$ called $\mu$-recursive. This class
contained
\begin{enumerate}[label=\alph*]
\item zero
\item successor
\item projections
\end{enumerate}

and was closed with respect to

\begin{enumerate}
\item composition;
\item primitive recursion;
\item minimalization, restricted to the case in wich the function that
  produces it is \emph{total}.
\end{enumerate}

Is clear that \[\mathcal{R}_0 \subsetneq \mathcal{R}\] and the
inclusion is strict, since:
\begin{itemize}
\item the functions in $\mathcal{R}_0$ are total;
\item some functions in $\mathcal{R}$ are partial.
\end{itemize}

Also \[\mathcal{R}_0 \subseteq \mathcal{R} \cap \text{Tot}\] but is
not obvious that the equality holds. In fact a function
$f \in \mathcal{R} \cap \text{Tot}$ can be total, but obtained trough
minimalization of partial functions. For example:
\[
  f(x,y) = \begin{cases}
    x+1 & x<y \\
    0 & x=y \\
    \uparrow & x>y
  \end{cases}
\]

\[
  \mu y . f(x,y) = \lambda x . x 
\]

Generally speaking is not obvious that if $f(x,y)$ is partial and
$\mu y . f(x,y)$ is total then \[\mu y . f(x,y) \in \mathcal{R}_0\]
but it is true!

\begin{theorem}
  \[\mathcal{R}_0 = \mathcal{R}\cap \text{Tot}\]
  \begin{proof}
    \begin{enumerate}
    \item[$(\subseteq)$] obvious.
    \item[$(\supseteq)$] Let $f \in \mathcal{R} \cap \text{Tot}$ so
      $f \in \mathcal{C}$ because of the proof of $\mathcal{R = C}$ so
      we can observe that
      \[f(\vec{x}) = C_p^1 ( \vec{x} , \mu t . J_p(\vec{x}, t))\]
      since composed of total functions, $f$ is total.
    \end{enumerate}
  \end{proof}
\end{theorem}
