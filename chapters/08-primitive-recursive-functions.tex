\chapter{Primitive recursive functions}
\begin{definition}[Primitive recursive functions]
  The class of
primitive recursive functions is the smallest set
$\mathcal{PR}$ containing
\begin{enumerate}[label=(\alph*)]
\item zero function
\item successor
\item projections
\end{enumerate}
and closed under
\begin{enumerate}[label=(\arabic*)]
\item composition
\item primitive recursion
\end{enumerate}
\end{definition}

One of the interesting property of $\mathcal{PR}$ is the
fact that primitive recursive functions are similar to \texttt{for} loops, while
function defined also by minimalisation are similar to \texttt{while} loops. This fact can be
formalized into a new variant on the URM, with \emph{structured
  programs}, where the jump instruction is substituted by
\texttt{for} and \texttt{while} loops. We'll call this machine
$\text{URM}_{\text{for,while}}$.

We can prove that this model has the same expressive power as the
URM model. This means that the class $\mathcal{C}_{\text{for,while}}$
coincides with $\mathcal{C}$ and $\mathcal{R}$,
while the class $\mathcal{C}_{\text{for}}$ of functions computable using only the \texttt{for}
construct coincides with $\mathcal{PR}$.

Thus, studying the relation between $\mathcal{R}$ and $\mathcal{PR}$ is
the same as studying the relation between the expressive power of
\texttt{for} and \texttt{while} constructs.  We know that many
``arithmetic'' functions, like $Pr(x), (x)_y, qt, mcm(x,y),
x^y$ are in $\mathcal{PR}$ and $\mathcal{PR}$ is closed under sum,
product and minimalisation. This class is very
extended, but it does not contain all computable functions, in other
words $\mathcal{PR} \subsetneq \mathcal{R}$, because $\mathcal{PR}$
functions are always total, since
$\mathcal{PR}$ functions are obtainable from base total functions by
composition and primitive recursion.

One could think that $\mathcal{PR}$ includes all the total recursive
functions, in other words if $Tot$ is the set of all total functions:
$\mathcal{PR} = \mathcal{R} \cap Tot$ [Hilbert, 1926].

This is false
($\mathcal{PR} \subsetneq \mathcal{R} \cap \text{Tot}$) because, even
if we restrain ourselves to total functions (programs that always
terminates), the \texttt{while} construct is essential.

\section{Ackermann's function}
Consider the function $ \psi: \nat^2 \rightarrow \nat $ defined as
\begin{equation*}
  \begin{cases}
    \psi(0,y) = y+1\\
    \psi(x+1,0) = \psi(x,1)\\
    \psi(x+1,y+1) = \psi(x, \psi(x+1, y))\\
  \end{cases}
\end{equation*}

This scheme univocally
guesses a function, because the value $\psi(x,y)$ is always defined
based on \emph{smaller} values of $\psi$ itself. But what does \emph{smaller} mean?

\begin{itemize}
  \item in $\psi(x+1,0) = \psi(x,1)$ the first argument
  diminishes.
  \item in $\psi(x+1,y+1)=\psi(x, \psi(x+1, y))$ at first we calculate $\psi(x+1, y)$
  where the second argument diminishes and then $\psi(x,u)$ in which the first argument
  $u$ is, the first argument is smaller.
\end{itemize}
 

We can see that the arguments diminish in a \emph{lexicographical}
order on $\mathbb{N}^2$, so we can define a partial ordered set 
$(\mathbb{N}^2, \leq_{lex} )$ with $(x,y) \leq (x', y')$
if $(x < x') \wedge (x=x'$ and $y \leq y')$ and then
$( \mathbb{N}^2, \leq_{lex} )$ is also \emph{well ordered}, i.e.
it does not allow for infinite descending sequences.

\begin{definition}[Partially ordered set]
  A set $D$ with a binary relation $\leq$ is a partially ordered set (poset) $(D, \leq)$ if $\leq$ is a partial order, i.e., for all $x,y,z\in D$, it is
  \begin{enumerate}
    \item reflexive: $ x\leq x $;
    \item antisymmetric: if $ x\leq y $ and $ y\leq x $, then $ x=y $;
    \item transitive: if $ x\leq y $ and $ y\leq z $, then $ x \leq z $.
  \end{enumerate}
\end{definition}

\begin{definition}[Well founded poset]
  $(D, \leq)$ is well-founded if every non-empty $X \subseteq D$ has a minimal element $d$, i.e.
  \[  
  \forall d' \in X \quad d' \leq d \Rightarrow d' = d
  \]
\end{definition}

\begin{observation}
  $(D, \leq)$ is well-founded iff it does not allow for
  infinite descending chains
  \[
    d_0 > d_1 > d_2 > \dots > d_n > d_n+1 \dots
  \]
\end{observation}
\newcommand{\conf}{\text{conf}} 
This fact can be useful when dealing
with termination problems. If we can conclude that the set of
configurations is well-founded, we simply need to prove
that for each step \( \conf _i \rightarrow \conf_{i+1} \) and
\( \conf _{i+1} < \conf _i \) to end our proof. This way our computation follows a descending sequence of values, necessarily finite.
In fact, if we think the calculation of $\psi$ is based on the
calculation of $\psi$ with smaller values, at some point it
will for sure reach the case $\psi(0,y) = y + 1$, terminating.

\begin{example}
  $(\nat^2, \leq_{lex})$ is \emph{well-founded}.
Let $\emptyset \neq X \subseteq \nat^2$ and define
\begin{align*}
  x_0 &= \min\{x \mid \exists y \in \nat . (x,y) \in X\} \\
  y_0 &= \min \{ y \; | \; (x_0,y) \in X\}
\end{align*}
then we can state that
$\min X = (x_0, y_0)$. In this way we can prove that the product of
two well-ordered sets is well-ordered.
\end{example}
\begin{observation}
  $\leq_{lex}$ is total.
\end{observation}

A stronger induction principle can be defined on natural numbers:
\begin{definition}[Complete induction]
  To prove that $\forall n\in\nat \;.\; P(n)$, we can assume 
  \[
    \forall n'<n\;.\; P(n')
    \]
  and prove $P(n)$.
\end{definition}
We can generalize the complete induction on $D$ well-founded poset:
\begin{definition}[Well-founded induction]
  Let $(D, \leq)$ be a well-founded poset and let $P(x)$ a
  property on elements of $D$. If for all $ d \in D $, assuming $P(d')$ for
  $d' < d$, we can conclude that $P(d)$ holds, then
  \[
    \forall d\in D. P(d)
  \]
\end{definition}

\newcommand{\nsqlex}{\( (\nat ^2, \leq_{lex})\)}

\begin{theorem}
  $\psi$ is total, i.e.
  \[\forall (x,y) \in \nat^2 \quad \psi (x,y) \downarrow \]
  \begin{proof}
    We proceed by well-founded induction on \nsqlex.
    Let $(x,y) \in \nat^2$, assume 
    \[
    \forall (x', y')\leq_{lex} (x,y) \;.\; \psi(x',y')\downarrow
    \]    
    we want to prove $\psi(x',y')\downarrow$.
    We have 3 cases:
    \begin{enumerate}
      \item[$(x=0)$] \(\psi(0,y) = y + 1 \downarrow\)
      \item[$(x>0,y=0)$] $\psi(x,0) = \psi(x-1,1) \downarrow $ for
          inductive hypothesis, since $(x-1,1) \leq_{lex} (x,0)$
      \item[$(x>0,y>0)$] $\psi(x,y) = \psi(x-1,\psi(x,y-1))$
          where $\psi(x,y-1) \downarrow$ by inductive hypothesis. Let
           $u = \psi(x,y-1)$, so
          $\psi(x,y) = \psi(x-1,u) \downarrow $ by inductive
          hypothesis.
      \end{enumerate}
  \end{proof}
\end{theorem}

\begin{exercise}
  Given a box with an arbitrary number of balls in it, each one with a
  number in $\nat$, do the following:
  \begin{itemize}
  \item extract a ball;
  \item substitute the extracted ball with an arbitrary number of
    balls, each one with a label lower than the extracted one.
  \end{itemize}
  Prove that this process always terminates.
\end{exercise}

\begin{theorem}
  $\psi$ is computable, i.e.
  \[\psi \in \mathcal{C} = \mathcal{R}\]
  \begin{proof}
    We can use the Church-Turing thesis to prove it: the computation
    of $\psi(x,y)$ is always reduced to the computation of $\psi$ on
    smaller input values.

    To do so, the definition of a valid set is needed. Intuitively a
    set $S \subseteq \nat^3$ is considered valid if, for all $(x,y,z) \in S$, we have
    \begin{itemize}
    \item $z = \psi(x,y)$
    \item $S$ contains all the
      triples needed to compute
      $\psi(x,y)$
    \end{itemize}
    \begin{example}
      $\psi(1,1) = \psi(0,\psi(1,0)) = \psi(0,\psi(0,1)) = \psi(0,2) = 3$

      $\Rightarrow S = {(1,1,3), (0,2,3), (1,0,2), (0,1,2)}$
    \end{example}
    Formally:
    \begin{definition}[Valid set]
      Let $S$ be a set of triples such that $S \subseteq \nat^3$. We say
      that $S$ is \emph{valid} if:
      \begin{enumerate}
      \item \((0,y,z) \in S \quad \Rightarrow \quad z = y+1\)
      \item \((x+1,0,z) \in S \quad \Rightarrow \quad (x,1,z) \in S\)
      \item \((x+1,y+1,z) \in S \quad \Rightarrow \quad \exists u \;.\; (x+1,
        y, u) \in S \wedge (x,u,z) \in S \)
      \end{enumerate}
    \end{definition}

    We can prove that for every $(x,y,z)\in\nat^3$ we have
   $\psi(x,y) = z$ if and only if there exists a valid \textbf{finite} set of triples $S \subseteq \nat^3$ 
    such that \((x,y,z) \in S\).
    
    by complete induction on
    $(x,y)$, leveraging the fact that the validity of a set is
    preserved under union (left as an exercise).

    \newcommand{\vnu}{\text{Val}(\nu)}

    A tuple $(x,y,z)$ can be encoded into an integer using the
    encoding function
    \[\Pi^3 \, : \, \nat^3 \rightarrow \nat \quad \quad (\Pi_i^3 \, : \,
      \nat \rightarrow \nat \text{ are the projections})\]. This way a
    set of tuples becomes a set of natural numbers
    $\{x_1, \dots, x_n\}$ that we can encode injectively:
    \[\{x_1, \dots, x_n\} \mapsto P_{x_1}, \dots, P_{x_n}\] Now we
    have $\nu$ the number that represents the set of tuples
    $S_\nu$. We can verify that
    \[(x,y,z) \in S_\nu \Longleftrightarrow P_{\Pi(x,y,z)} \; | \;
      \nu \] and the predicate $\vnu=$``$\nu$ encodes a
    set of valid tuples'' is decidable.

    In fact $\vnu$ is true if and only if:
    \begin{itemize}
    \item \(\forall i \leq \nu \quad (\nu)_i = 1\)
    \item{
        \( \forall \omega \leq \nu \quad P_{\omega \; | \; \nu}\) \\
        \[\Rightarrow \begin{cases}
            \Pi_1(\omega) = 0 \quad & \Rightarrow
            \quad \Pi_3(\omega) = \Pi_2(\omega) + 1 \\
            \Pi_1(\omega) > 0 \quad & \Rightarrow \quad
            \begin{cases}
              \Pi_2(\omega) = 0 & \Rightarrow
              \Pi(\Pi_1(\omega),0,\Pi_3(\omega)) \in S_\nu \\
              \Pi_2(\omega) > 0 & \Rightarrow
              \exists u \leq \omega \text{ s.t. } \\
              & \quad \Pi(\Pi_1(\omega), \Pi_2(\omega)-1,u) \in S_\omega \\
              & \quad \Pi(\Pi_1(\omega)-1, u,z) \in S_\omega
            \end{cases}
            \\
          \end{cases}
        \]
      }
    \end{itemize}

    And the associated caratheristic function
    \[\chi_{\text{Val}} \in \mathcal{PR}\]

    We can also verify that:

    \[
      R(x,y,z) = \begin{cases}

        \chi_{\text{Val}(\omega)} & \text{if $\omega$ encodes for some
          valid $S$ that contains $(x,y,z)$ for some $z$} \\

        0 & \text{otherwise}

      \end{cases}
    \]

    \[
      R(x,y,z) = \chi_{\text{Val}(\omega)} \cdot \text{sg} (\omega + 1
      \dot{-} \mu z \leq \omega . \text{div}(P_{\Pi(x,y,z)}, \omega)
    \]

    this way we can write the Ackermann function as

    \[
      \psi(x,y) = \mu (z,y) \; \cdot \; \overline{\text{sg}}(R(x,y,\omega)
      \cdot \text{div}(P_{\Pi(x,y,z)}, \omega))
    \]

    So since it is computable,

    \[
      \psi \in \mathcal{R} = \mathcal{C}
    \]

  \end{proof}
\end{theorem}

\begin{theorem}
  \[ \psi \notin \mathcal{PR} \]
  \begin{proof}
    The proof of the fact that $\psi$ is not a primitive recursive
    function is done by showing that $\psi$ \emph{grows} faster than
    every function in $\mathcal{PR}$. We already saw how we obtain
    \begin{itemize}
    \item sum from successor
    \item product from sum
    \item exponential from product
    \end{itemize}
    each one by nested primitive recursion.

    The idea of the Ackermann function is that it won't be possible to compute it with a finite number of
    nested primitive recursions.

    In fact, by calling \[\psi_x(y) = \psi(x,y)\] we have
    that
    \[
      \psi_{x+1}(y) = \psi_x(\psi_{x+1}(y-1)) =
      \psi^2_{x}(\psi_{x+1}(y-2)) = \dots = \psi_x^{y+1}(1)
    \]
    \begin{align*}
      \psi_0(y) &= y+1 = succ(x)\\
      \psi_1(y) &= \psi_0^{y+1}(1) = y+2\\
      \psi_2(y) &= \psi_1^{y+1}(1) = 2y+3\\
      \psi_3(y) &= \psi_2^{y+1}(1) = 2^{y+3}-3
    \end{align*}

    e.g.

    \(\psi_0(1) = 2, \quad \psi_1(1) = 3, \quad \psi_2(1) = 5, \quad
    \psi_3(1) = 13, \quad \dots\)

    Intuitively, if $x$ grows so does the level of nesting
    in the functions, which is equivalent to say that we need more\texttt{for} loops. 
    Since $x$ can grow to infinity and
    \texttt{for} loops cannot be nested to infinity, a \texttt{while}
    loop is needed. More precisely, given a
    function $f :\nat^n \to \nat \in \mathcal{PR}$ and a program $P$ computing $f$ using only
    \emph{for-loops} (primitive recursion),
    if $j$ is the maximum level of nesting of \texttt{for}-loops, then
    \[
      P(x_1, \dots, x_k)\downarrow \quad \text{in a number of steps }
      < \psi_{j+1}(\max\{x_1, \dots, x_k\})
    \]
    in this way we see that $\psi_j$ gives a bound to the time complexity
    as well to the number of increment operations, so
    \[f(\vec{x}) < \psi_{j+1}(\max\{x_1, \dots, x_k\})\]

    Now, assume $\psi \in \mathcal{PR}$, let $j$ be the
    level of nesting of \texttt{for}-loops for computing $\psi$, so 
    \[\forall (x,y)\;.\;\psi(x,y) < \psi_{j+1}(\max\{x,y\})\]
    Let $x=y=j+1$ big enough we have
    that \[\psi(j+1,j+1) < \psi_{j+1}(j+1) = \psi(j+1,j+1)\] which is
    absurd, so $\psi \notin \mathcal{PR}$.
  \end{proof}
\end{theorem}

\begin{observation}
Initially, Gödel and Kleene studied a class
of functions $\mathcal{R}_0$ called $\mu$-recursive. This class
contained
\begin{enumerate}[label=\alph*]
\item zero function
\item successor
\item projections
\end{enumerate}
and was closed under
\begin{enumerate}
\item composition;
\item primitive recursion;
\item minimization, restricted to the case in which the function that
  produces is \emph{total}.
\end{enumerate}
$\mathcal{R}_0 \subset \mathcal{R}$ trivially holds, since:
\begin{itemize}
\item functions in $\mathcal{R}_0$ are total;
\item some functions in $\mathcal{R}$ are partial.
\end{itemize}

Also \[\mathcal{R}_0 \subseteq \mathcal{R} \cap \text{Tot}\] but is
not obvious that the equality holds. In fact, a function
$f \in \mathcal{R} \cap \text{Tot}$ can be total, but obtained through
minimization of partial functions. For example:
\begin{align*}
  f(x,y) &= \begin{cases}
    x+1 & x<y \\
    0 & x=y \\
    \uparrow & x>y
  \end{cases} \\
  \mu y . f(x,y) &= \lambda x . x
\end{align*}
thus, $f(x,y)$ is partial and
$\mu y . f(x,y)$ is total, then \[\mu y . f(x,y) \in \mathcal{R}_0\]
\begin{theorem}
  \[\mathcal{R}_0 = \mathcal{R}\cap \text{Tot}\]
  \begin{proof}
    \begin{enumerate}
    \item[$(\subseteq)$] trivial.
    \item[$(\supseteq)$] Let $f \in \mathcal{R} \cap \text{Tot}$, then
      $f \in \mathcal{C}$.
      We can observe that
      \[f(\vec{x}) = c_P^1 ( \vec{x} , \mu t . j_P(\vec{x}, t))\]
      but $c_P^1, j_P$ are total, so $f$ is total.
    \end{enumerate}
  \end{proof}
\end{theorem}
\end{observation}


