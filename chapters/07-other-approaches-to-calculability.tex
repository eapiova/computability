\chapter{Other approaches to computability}
We already observed that the URM machine is just one of the many possible computational models that allow us to formalize the notion of computable functions.

We could have used:
\begin{itemize}
\item Turing machine
\item Canonical deduction systems of Post
\item $\lambda$-calculus of Church
\item Partial recursive functions of Gödel-Kleene
\end{itemize}

All of these approaches define the \textbf{same class of computable functions}, leading to the

\textbf{Church-Turing thesis}: a function is computable through an effective procedure 
if and only if it is URM-computable

Now, we introduce another formalism for the definition of computable functions, the set $\mathcal{R}$ of \textbf{partial recursive functions} of Gödel-Kleene and prove that it is equivalent to the URM, meaning it defines the same class of functions: $\mathcal{R} = \mathcal{C}$.

\section{Partially recursive functions}

\begin{definition}
  The class $ \mathcal{R} $ of \textbf{partially recursive functions} is the least class of partial functions on the natural numbers which contains
  \begin{enumerate}[label=(\alph*)]
    \item zero function;
    \item successor;
    \item projections
    \end{enumerate}
    
    and \textbf{closed} under
    \begin{enumerate}
    \item composition;
    \item primitive recursion;
    \item minimalisation.
    \end{enumerate}
\end{definition}

It is a well given definition.

\begin{definition}[Rich class]
    A class of functions $\mathcal{A}$ is said to be \textbf{rich} if it includes (a),(b) and (c) and it's closed under (1), (2) and (3).
\end{definition}

$\mathcal{R}$ is rich and for all $\mathcal{A}$, we have $\mathcal{R}\subseteq\mathcal{A}$

We observe that the property of being ``rich class'' is \textbf{closed for intersection}:

Let $\{\mathcal{A}_i\}i\in I$ a family of rich classes, then $\bigcap_{i\in I}\mathcal{A}_i$ rich.

And finally we define:

\textbf{Definition} The set of the partially recursive functions is:

$\mathcal{R} = \bigcap_{\mathcal{A} \subseteq \bigcup_k \nat^k\rightarrow\nat
  \land
  \mathcal{A}rich} \mathcal{A}$

\textbf{Note} $\mathcal{R}$ admits an inductive characteristic\\
$\mathcal{R}_0 = \{a,b,c\}$\\
$\mathcal{R}_{i+1} = \mathcal{R}_i \cup \{1,2,3 $ applied to $ R_i\}\\
R = \bigcup_i \mathcal{R}_i$

Another interesting class, on which we will come back:

\textbf{Definition} (Primitive recursive functions).

The class of the primitive recursive functions is the least class of functions $\mathcal{PR}\subseteq \bigcup_k\nat^k\rightarrow\nat$ that contains (a),(b) and (c) and is closed for (1) and (2).

\begin{theorem}[$\mathcal{R} = \mathcal{C}$]\label{reqc} The partially recursive
  functions coincide with those URM-computable
  
  \begin{proof}
    $ \mathcal{R} \subseteq \mathcal{C} $ because $\mathcal{R}$ is the
    least rich class, the other is a rich class
    $\Rightarrow \mathcal{R}\subseteq\mathcal{C}$.

    Let us now consider $ \mathcal{C} \subseteq \mathcal{R} $. Let
    $ f:\nat^k\rightarrow\nat \in \mathcal{C} $ be a function and show
    that $ f \in \mathcal{R} $. We know that $ \exists P \in URM $
    programs $ f_P^{(k)} = f$.

    Considering the following functions (dependant on $P$)

    With $ C_p^1(\vec{x}, t) $ we indicate contents of register 1 after $t$ steps of $ P(\vec{x}) $. A computation step is the execution of an instruction.

    It is understood that if $P(\vec{x})$ terminates in less than $t$ steps, $ C_p^1(\vec{x}, t) $ gives the content of $R_1$ in the final configuration.

    With $ J_P(\vec{x},t) $ we indicate instruction to be executed at the $t+1$-th step (after $t$ steps) of $P(\vec{x})$ (program counter) If the program has already ended, it is worth 0.

    Clearly $C_p^1$ and $J_p$ are total functions (we will need this later\dots)

    If $ f(\vec{x})\downarrow $ then $ P(\vec{x})\downarrow $ after $ t_0 $ pass, $ t_0 = \mu t. J_P(\vec{x},t) \Rightarrow f(\vec{x}) = C_p^1(\vec{x},t_0) = C_P^1(\vec{x}, \mu t.J_P(\vec{x},t)) $.
    Otherwise, if $ f(\vec{x})\uparrow $ then $P(\vec{x})\uparrow$ too and $ \mu t.J_P(\vec{x},t)\uparrow $

    Therefore:
    $f(\vec{x}) = c_p^1(\vec{x}),\mu t.J_p(\vec{x},t)$

    % =======================================================

    If you knew that $ C_P^1, J_P \in \mathcal{R} $ then here it is shown that $ f \in \mathcal{R} $

    We prove that they are even in $ \mathcal{PR} $

    The idea of the proof is the following:

    \begin{itemize}
    \item we can work on sequences encodings taht rapresents the registers and program counter configuraiton
    \item we then manipulate such sequences  with the funcitons (\( p_x, q_t, \text{div}, \dots \) ) that we built by:
      \begin{itemize}
      \item composition
      \item primitive recursion
      \end{itemize}
    \item this way we obtain $C_p^1, J_p$ through primitive recursion
    \end{itemize}
    More precisely, the computation state is a tuple
    $\langle \vec{R}, t \rangle$ where $\vec{R}$ are the registers of the
    machine and $t$ is the current instruction.

    To a register configuration in which a finite number of registers
    contains a value other than 0 can be encoded with

    \begin{center}
      $\text{cod}(\vec{R}) = \prod\limits_{i \geq 1}p_i^{r_i}$
    \end{center}

    where just a finite number of factors is $\neq 1$. At this point,
    given $x \geq 1$ interpreted as $\text{cod}(\vec{R})$

    \begin{center}
      $r_i = (x)_i$
    \end{center}

    Using this encoding, we can consider the function $C_p(\vec{x},t)$
    (the registers' configuration after t steps of $P(\vec{x})$) as a
    function $C_p : \nat^{k+1} \rightarrow \nat$. This way we can define:

    \begin{center}
      $\sigma_p(\vec{x},t) = \langle C_p(\vec{x},t), J_p(\vec{x},t) \rangle$
    \end{center}

    the state of the computation of $P(\vec{x})$ after $t$ steps. And
    using the encoding function for the $\pi$ couples we can view
    $\sigma_p$ as:

    $\sigma_p : \nat^{k+1} \rightarrow \nat$

    $\sigma_p(\vec{x}, t) = \pi(C_p(\vec{x},t), J_p(\vec{x},t))$

    and we can define it with the primitive recursion:

    $\sigma_p(\vec{x}, 0) = \pi(\prod\limits_{i=1}^k p_i^{x_i}, 1)$

    $\sigma_p(\vec{x}, t+1) = \pi(C_p(\vec{x},t+1), J_p(\vec{x},t+1))$

    with

    \[
      C_p(\vec{x},t+1) = \begin{cases}
        qt(p_n^{(C_p(\vec{x},t))_n}, C_p(\vec{x},t)) & \quad \text{if }1 \leq J_p(\vec{x},t) \leq s \; \And \; I_{J_p(\vec{x},t)} = z(n) \\
        p_n \cdot C_p(\vec{x},t) & \quad \text{if }1 \leq J_p(\vec{x},t) \leq s \; \And \; I_{J_p(\vec{x},t)} = s(n) \\
        qt(p_n^{(C_p(\vec{x},t))_n}, C_p(\vec{x},t)) \cdot p_n^{(C_p(\vec{x},t))_m} & \quad \text{if }1 \leq J_p(\vec{x},t) \leq s \; \And \; I_{J_p(\vec{x},t)} = T(m,n) \\
        C_p(\vec{x}, t) & \quad \text{otherwise}
      \end{cases}
    \]

    \[
      J_p(\vec{x}, t+1) = \begin{cases}
        J_p(\vec{x}, t) + 1 & \quad \text{if } 1 \leq J_p(\vec{x}, t) < s \And I_{J_p(\vec{x}, t)} = z(n), s(n), T(m,n) \\
        & \quad \quad \text{ or } J(m,n,q) \text{ with } (\sigma_p(\vec{x}, t))_m \neq (\sigma_p(\vec{x}, t))_n \\
        q & \quad \text{if } 1 \leq J_p(\vec{x}, t) < s \And I_{J_p(\vec{x}, t)} = J(m,n,q) \text{ with } q\leq 5 \\
        & \quad \quad \And (C_p(\vec{x}, t))_m = (C_p(\vec{x}, t))_n \\
        0 & \quad \text{otherwise}
      \end{cases}
    \]

    this way we define by primitive recursion $\sigma_p$, even if not in
    detail, starting from functions in $\mathcal{R}$.

    $\sigma_p \in \mathcal{R}$

    \textbf{Observation:} all the functions we used are in  $\mathcal{PR} \Rightarrow \sigma_p \in \mathcal{PR}$

    $C_p^1(\vec{x}, t) = (\pi_1(\sigma_p(\vec{x}, t)))_1 \in \mathcal{R}$

    $J_p(\vec{x}, t) = \pi_2(\sigma_p(\vec{x}, t)) \in \mathcal{R}$

    so $f$, defined by composition and minimization by $C_p^1, J_p$ is in $\mathcal{R}$ as we wanted.

  \end{proof}
\end{theorem}
