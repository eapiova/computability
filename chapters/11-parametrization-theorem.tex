\chapter {Parametrisation theorem}
\newcommand{\smn}{$S_n^m$}

We'll start by giving an intuition on what the theorem talks
about. Let $f : \nat^2 \rightarrow \nat$ be a computable
function, there exists $e \in \nat$ such that
 \[f(x,y) = \varphi_e^{(2)}(x,y)\]

Now, if we fix the first argument to a certain value $x \in \nat$, we obtain a
function of a single argument $f_x : \nattonat$
\[f_x(y) = f(x,y) = \varphi_e^{(2)}(x,y)\] and for all $ x \in \nat, \; f_x$
is computable (since it is obtained as composition of computable
functions). This means that there exists a $d \in \nat$ such that
\[f_x = \varphi_d\] in other words, for all $ y \in \nat$
\[f_x(y) = \varphi_e^{(2)}(x,y) = \varphi_d(y)\] Clearly $d$
depends on $e$ and $x$, so we can consider a with 
total function $s : \nat^2 \rightarrow \nat$
\[
  s(e, x) = d
  \]
The \textit{smn} theorem tells us that $s$ is computable. Intuitively,
how can we compute $s(e, x)$?
\begin{itemize}
\item get the program
  $P_e = \gamma^{-1}(e)$ that computes $\varphi_e^{(2)}(x,y)$
\item get the program that computes $f_x = \lambda y \; . \; f(x,y)$ with
  fixed $x$, from $P_e$:
  \begin{itemize}
  \item move $y$ to $R_2$;
  \item write $x$ on $R_1$;
  \item execute $P_e$
  \end{itemize}
\item take the code of the obtained program
\end{itemize}
Functions on indices, like $s$, are functions that
transform programs. The \emph{smn} theorem states that the operation of
fixing an argument of a program is effective.

\begin{example}
  Consider the computable function 
  \[f(x,y) = x^y\]
  We know that an
  index such that \(\varphi_d = f_x\) must exist, in other
  words \[\varphi_d(x,y) = f(x,y) = x^y\] So, when $x$ varies we
  obtain computable functions
  \begin{itemize}
  \item[] \(f_0(y) = y^0 = 1 \quad \mbox{ with index} \quad s(e,0)\)
  \item[] \(f_1(y) = y^1 = y \quad \mbox{ with index} \quad s(e,1)\)
  \item[] \(f_2(y) = y^2  \quad \mbox{ with index} \quad s(e,2)\)
  \item[] \(\dots\)
  \end{itemize}
  by \emph{smn} theorem we can determine those indices in an
  effective way. The theorem also does this in general for functions
  of the form $f(\vec{x}, \vec{y}) : \nat^{m+n} \rightarrow \nat$,
  which give the name to the theorem.
\end{example}

\section{\emph{smn} Theorem}
\begin{theorem}[\emph{smn} theorem]
  Given $m, n \geq 1$ there is a computable total function
  \[s_{m,n} : \nat^{m+1} \rightarrow \nat\] such that 
  \[
    \varphi_e^{(m+n)}(\vec{x},\vec{y}) = \varphi_{s_{m,n}(e,
      \vec{x})}^{(n)}(\vec{y}) \quad \forall e \in \nat, \; \vec{x} \in
    \nat^m, \vec{y} \in \nat^n
  \]
  \begin{proof}
    intuitively, given $e \in \nat$, $\vec{x}\in \nat^m$
    \begin{itemize}
    \item we get the program
      $P_e = \gamma^{-1}(e)$ in standard form that computes $\varphi_e^{(m+n)}$,
      so starting from
      \[
        \begin{tabu}{|c|c|c|c|c}
          \hline
          \vec{x} & \vec{y} & 0 & 0 & \dots \\ \hline
        \end{tabu}
        \quad \quad \mbox{it computes }
        \varphi_e^{(m+n)}(\vec{x},\vec{y})
      \]
    \item from $P_e$ we can build a new program $P'$.
      Starting from
      \[
        \begin{tabu}{|c|c|c|c}
          \hline
          \vec{y} & 0 & 0 & \dots \\ \hline
        \end{tabu}
        \quad \quad \mbox{it computes }
        \varphi_e^{(m+n)}(\vec{x},\vec{y})
      \]
    \end{itemize}

    In fact, it is sufficient to
    \begin{itemize}
    \item move $\vec{y}$ forward of $m$ registers
    \item load $\vec{x}$ in the free $m$ registers
    \item execute $P_e$
    \end{itemize}

    The program $P'$ can be
    \begin{center}
      \begin{tabular}{lr}
        $T(1, m+1)$               &          \\
        $\dots$                   &          \\
        $T(n, m+n)$               &          \\
        $z(1)$                    &          \\
        $s(1)$                    &          \\
        $\dots$                   & \comment{$x_1$ times} \\
        $z(m)$                    &          \\
        $s(m)$                    &          \\
        $\dots$                   & \comment{$x_m$ times} \\
        $P_e$                     & 
      \end{tabular}
    \end{center}
    \begin{remark}
      The concatenation has to update all the jump
    instructions in $P_e$,
    $J(m^\prime, n^\prime, t) \rightsquigarrow J(m^\prime, n^\prime, t
    + m + n + \sum_{i=1}^mx_i)$
    \end{remark}

    Built $P$, we have
    \[s(e, \vec{x}) = \gamma(P')\] 
    Each function and construction
    method used are effective (so are $\gamma, \gamma^{-1}$).
    Thus, the existence, totality and computability of $s$ are
    informally proven, by appealing to the Church-Turing
    thesis.

    The formal proof of calculability is long, but not difficult.
    \subsection*{Update function}
      \newcommand{\up}[1]{\ensuremath{\mathit{upd}({#1})}}
      Consider
      \[
        \mathit{upd} : \nat^2 \rightarrow \nat
      \]
      where $\up{e, h}$ is the code of a program obtained from
      $P_e = \gamma^{-1}(e)$
     by updating each jump instruction $J(m,n,t)$ to $J(m,n,t+h)$.
      
     It is useful to define a support function that works
      on each single instruction encoded with $\beta$
      \newcommand{\tup}[1]{\ensuremath{\tilde{\mathit{upd}}({#1})}}
      \[
        \tilde{\mathit{upd}} : \nat^2 \rightarrow \nat
      \]
      where $\tup{i, h}$ is the code of the instruction $\beta^{-1}(i)$,
       updated if it is jump instruction.

      \newcommand{\qt}[1]{\ensuremath{\mathit{qt}({#1})}}
      \newcommand{\rmf}[1]{\ensuremath{\mathit{rm}({#1})}}

      Given $i,h \in \nat$ and $q=\qt{4,i}, r=\rmf{4,i}$ it is formally defined in this way
      \begin{align*}
        \tup{i,t} &= \begin{cases}
          4 * \nu(\nu_1(q), \nu_2(q), \nu_3(q) + h) + 3 & r=3 \\
          i & r \neq 3
        \end{cases} \\
        &= sg(r - 3) \cdot i + \bar{sg}(r - 3) \cdot (4 * \nu(\nu_1(q), \nu_2(q), \nu_3(q) + h) + 3)
      \end{align*}
      Now
      \begin{align*}
        \up{e,t} &= \tau(\tup{a(e,1), h}, \dots, \tup{a(e, l(e)), h}) \\
        &= \left(\prod^{l(e)-1}_{i=1}p_i^{\tup{a(e,i), h}}\right) \cdot p_{l(e)}^{\tup{a(e,l(e)), h} + 1}-2
      \end{align*}

\subsection*{Concatenation of sequences}
We will need a function
    \newcommand{\conc}[1]{\ensuremath{\mathit{c}({#1})}}
    \[
      \mathit{c}: \nat^2 \rightarrow \nat
    \]
    to concatenate sequences
    \[
        \conc{e_1, e_2} = \tau(a(e_1,1), \dots, a(e_1,l(e_1)),
        a(e_2,1), \dots, a(e_2, e(e_2)))
    \]
\subsection*{Concatenation of programs}
    \newcommand{\seq}[1]{\ensuremath{\mathit{seq}({#1})}}
    \[
      \mathit{seq} : \nat^2 \rightarrow \nat
    \]
where
    \[
      \seq{e_1, e_2} = \gamma \left( \begin{array}{c}
                                P_{e_1} \\
                                P_{e_2}
                              \end{array} \right)
                              = \conc{e_1, \up{e_2, l(e_2)}}
    \]

\subsection*{Transfer}
    \newcommand{\tran}[1]{\ensuremath{\mathit{transf}({#1})}}
    \[
      \mathit{transf}: \nat^2 \rightarrow \nat
    \]
where
\begin{align*}
\tran{m,n} &= \gamma(T([1,n],[m+1, m+n])) \\
&= \tau(\beta(T(1,m+1)), \dots, \beta(T(n,n+m))) 
\end{align*}

\subsection*{Set}
    \newcommand{\set}[1]{\ensuremath{\mathit{set}({#1})}}
    \[
      \mathit{set} : \nat^2 \rightarrow \nat
    \]
\begin{align*}
  \set{i,x} &= \gamma\left( \begin{array}{c}
    z(i) \\
    s(i) \\
    \vdots \\
    s(i)
  \end{array} \right) \\
&= \tau(\beta(z(i)), \beta(s(i)), \dots, \beta(s(i))) \\
\end{align*}

\subsection*{Proof}
    At this point, with
    \newcommand{\pref}[1]{\ensuremath{\mathit{pref}_{m,n}({#1})}}
    \[
      \mathit{pref}_{m,n}: \nat^m \rightarrow \nat
    \]
where
    \[
      \pref{\vec{x}} = \seq{\tran{m,n}, \seq{\set{1,x_1},\dots, \seq{\dots , \set{m,x_m}}} \dots}
    \]
    we have that
    \[
      s_{m,n}: \nat^{m+1} \rightarrow \nat
    \]
    \[
      s_{m,n}(e, \vec{x}) = \seq{\pref{\vec{x}}, e}
    \]
    which is $\in \pr$
  \end{proof}
\end{theorem}

\subsection*{\emph{smn} Theorem applications}
\begin{observation}
  We proved that \emph{smn} is not just computable and total,
but is also primitively recursive. 
\end{observation}

Furthermore, the theorem is usually presented
in the following simpler shape
\begin{corollary}[Simplified \emph{smn} theorem]\label{corollary:simple-smn}
  Let $f: \nat^{m+n} \rightarrow \nat$ be a computable function. There
  exists a total computable function $s: \nat^m\rightarrow \nat$ such that
  \[
    f(\vec{x}, \vec{y}) = \varphi_{s(\vec{x})}^{(n)}(\vec{y}) \quad \quad
    \forall x \in \nat^m \quad \forall y \in \nat^n
  \]
  \begin{proof}
    Since $f$ is computable, given $e \in \nat$ and $s(\vec{x}) = s_{m,n}(e, \vec{x})$
    \begin{align*}
      f(\vec{x}, \vec{y}) &= \varphi_e^{(m+n)}(\vec{x}, \vec{y}) \\
      &= \varphi_{s_{m,n}(e, \vec{x})}^{(n)}(\vec{y})\\
      &= \varphi_{s(\vec{x})}^{(n)}(\vec{y})
    \end{align*}
  \end{proof}
\end{corollary}

\begin{example}
  Prove that there exists a total computable function $k : \nattonat$ such that
  for all $n, x \in \nat$
  \[
    \varphi_{k(n)}(x) = \lfloor \sqrt[n]{x} \rfloor
  \]
  It means that $k$ is an enumeration of functions of the form
  $\lfloor \sqrt[n]{x} \rfloor$ or $k$ is a function that given
  $n$, it returns the program that computes $\lfloor \sqrt[n]{x}
  \rfloor$.
\begin{proof}
  We define $f : \nat^2 \to \nat$
\begin{align*}
  f(n,x) = \lfloor \sqrt[n]{x} \rfloor &= \mu y \leq x \quad \mbox{``}(y+1)^n > x\mbox{''} \\
      &= \mu y \leq x \; . \; (x+1 \dotdiv (y+1)^n)
\end{align*}
  the function $f$ is computable because it is a bounded minimalisation of a composition of known computable functions.
  By corollary
  \ref{corollary:simple-smn}, there exists $k: \nattonat$ total computable such that for all $n, x \in \nat$
  \[
    \varphi_{k(n)}(x) = f(n,x) = \lfloor \sqrt[n]{x} \rfloor
  \]
\end{proof}
\end{example}

\begin{example}
  There exists $k : \nattonat$ computable and total such that for all $n \in \nat$,
  $\varphi_{k(n)}$ is defined only on $n^{th}$
  powers, i.e.
  \[
    W_{k(n)} = \{ \; x \; | \; \exists y \in \nat \ . \ x = y^n \; \}
  \]
\begin{proof}
We define $f : \nat^2 \rightarrow \nat$ as
  \begin{align*}
    f(n,x) &= \begin{cases}
      \sqrt[n]{x} & \quad \mbox{if } \exists y \in \nat \ . \ x = y^n \\
      \uparrow &\quad \mbox{otherwise}
    \end{cases} \\
    & = \mu y \; . \; |y^n-x|
  \end{align*}
  It is computable. By corollary
  \ref{corollary:simple-smn}, there exists $k: \nattonat$ total computable such that for all $n, x \in \nat$
  \[
    \varphi_{k(n)}(x) = f(n,x)
  \]
  We claim 
  \[
    W_{k(n)} = \{ \; x \; | \; \exists y \in \nat \ . \ x = y^n \; \}
  \]
  in fact, $x \in W_{k(n)}$ iff
  $\varphi_{k(n)}(x)\downarrow$ iff $f(n,x)\downarrow$ iff $x$ is the
  $n^{th}$ power.
\end{proof}
\end{example}

\begin{exercise}
  Prove that there exists a function $s: \nattonat$ which is total and computable
  such that
  \[W_{s(x)}^{(k)} = \{(y_1, \dots, y_k) \; | \;
    \sum\limits_{i=1}^ky_i = x)\}\]

  \textbf{Idea:} define
  \begin{align*}
    f(x, \vec{y}) &=
      \begin{cases}
        0 & \quad \sum_{i=1}^ky_i = x \\
        \uparrow & \quad \mbox{otherwise}
      \end{cases} \\
      &= \mu z \; . \; \left|\left(\sum_{i=1}^ky_i\right) - x\right|
  \end{align*}
\end{exercise}
